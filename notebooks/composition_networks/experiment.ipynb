{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "class CompositionNetworks():\r\n",
    "\r\n",
    "    def load_corpus():\r\n",
    "        filepath = \"C:/Users/rahin/projects/nlp-pos-bioes-tagging/data/corpus/\"\r\n",
    "        list_of_files = [\"02-21.10way.clean.txt\",\"22.auto.clean.txt\",\"23.auto.clean.txt\"]\r\n",
    "        list_of_sentences = []\r\n",
    "        for file in list_of_files:\r\n",
    "            f = open(filepath+str(file),\"r\")\r\n",
    "            content = f.readlines()\r\n",
    "            list_of_sentences.append(content)\r\n",
    "        return list_of_sentences\r\n",
    "\r\n",
    "corpus_dirty = CompositionNetworks.load_corpus()\r\n",
    "\r\n",
    "corpus_less_dirty = []\r\n",
    "\r\n",
    "for corpus in corpus_dirty:\r\n",
    "    print(len(corpus))\r\n",
    "    for line in corpus:\r\n",
    "        corpus_less_dirty.append(line)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39832\n",
      "1700\n",
      "2416\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "list1 = [5, 6, 7]\r\n",
    "list2 = [0, 1, 2]\r\n",
    "sum_list = [a + b for a, b in zip(list1, list2)]\r\n",
    "sum_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[5, 7, 9]"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# one example!\r\n",
    "example = corpus_less_dirty[3]\r\n",
    "example"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'(TOP (S (NP (DT The) (NN luxury) (NN auto) (NN maker)) (NP (JJ last) (NN year)) (VP (VBD sold) (NP (CD 1,214) (NNS cars)) (PP (IN in) (NP (DT the) (NNP U.S.))))))\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# bifurcate the dataset depending on the size of the syntax trees:\r\n",
    "# Plan: Only take sentences that has low to medium complexity\r\n",
    "how_big = []\r\n",
    "for sentence in corpus_less_dirty:\r\n",
    "    how_big.append(sentence.replace(' (. .)))\\n','').split(' ')[-1].count(')'))\r\n",
    "\r\n",
    "# for idx, item in enumerate(how_big[:5]):\r\n",
    "#     if item == 4:\r\n",
    "#         print(idx)\r\n",
    "# import collections\r\n",
    "# counter = collections.Counter(how_big)\r\n",
    "# print(counter)\r\n",
    "\r\n",
    "syntax_tree_size_1, syntax_tree_size_2, syntax_tree_size_3, syntax_tree_size_4, syntax_tree_size_5 = [], [], [], [], []\r\n",
    "\r\n",
    "for idx, sentence in enumerate(corpus_less_dirty):\r\n",
    "    curr_tree_size = sentence.replace(' (. .)))\\n','').split(' ')[-1].count(')')\r\n",
    "    if curr_tree_size == 1:\r\n",
    "        syntax_tree_size_1.append(sentence)\r\n",
    "    elif curr_tree_size == 2:\r\n",
    "        syntax_tree_size_2.append(sentence)\r\n",
    "    elif curr_tree_size == 3:\r\n",
    "        syntax_tree_size_3.append(sentence)\r\n",
    "    elif curr_tree_size == 4:\r\n",
    "        syntax_tree_size_4.append(sentence)\r\n",
    "    elif curr_tree_size == 5:\r\n",
    "        syntax_tree_size_5.append(sentence)\r\n",
    "    else:\r\n",
    "        continue\r\n",
    "\r\n",
    "print(f\"One example sentence of size 1: {syntax_tree_size_1[2]}\")\r\n",
    "print(f\"One example sentence of size 4: {syntax_tree_size_4[2]}\")\r\n",
    "print(how_big[:5])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "One example sentence of size 1: (TOP (NP (DT No) (NN wonder) (. .)))\n",
      "\n",
      "One example sentence of size 4: (TOP (S (NP (NNP Richard) (NNP Stoltzman)) (VP (VBZ has) (VP (VBN taken) (NP (DT a) (NN gentler) (, ,) (ADJP (RBR more) (JJ audience-friendly)) (NN approach)))) (. .)))\n",
      "\n",
      "[5, 3, 10, 6, 9]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# option 2: take NP, VP, ADVP etc like so:\r\n",
    "\r\n",
    "print(example)\r\n",
    "example_entities = example.split(' ')\r\n",
    "# (example_entities)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(TOP (S (NP (DT The) (NN luxury) (NN auto) (NN maker)) (NP (JJ last) (NN year)) (VP (VBD sold) (NP (CD 1,214) (NNS cars)) (PP (IN in) (NP (DT the) (NNP U.S.))))))\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "def identify_phrases_size_n(find_chunks_from, chunk_type, chunk_size):\r\n",
    "\r\n",
    "    #collects NP phrases of size 2 and 1 (at the end of sentence)\r\n",
    "    list_of_phrases = []\r\n",
    "    n = chunk_size \r\n",
    "    for sentence in find_chunks_from: # iterate through all sentences\r\n",
    "        curr_sentence = sentence.split(' ')\r\n",
    "        \r\n",
    "        for idx, item in enumerate(curr_sentence): \r\n",
    "            # print(item)\r\n",
    "\r\n",
    "            # look if NP phrase exist in this sentence            \r\n",
    "            if item == chunk_type:\r\n",
    "                NP_item_position = idx\r\n",
    "                if '))' in curr_sentence[idx:idx+n][-1]:\r\n",
    "                    curr_phrase = curr_sentence[idx:idx+n] # n= 5\r\n",
    "                    \r\n",
    "                else: continue #skipping chunks longer than 2 words\r\n",
    "                \r\n",
    "                # remove the end of sentence indicator:\r\n",
    "                if curr_phrase[-1] == '.)))\\n':\r\n",
    "                    curr_phrase =  curr_phrase[:-2]\r\n",
    "\r\n",
    "                list_of_phrases.append(tuple(curr_phrase))             \r\n",
    "            \r\n",
    "            # elif item != chunk_type: # skip until the next NP phrase in the same sentence\r\n",
    "            #     continue\r\n",
    "        # print('-'*100)\r\n",
    "    return list_of_phrases\r\n",
    "    \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "\r\n",
    "# NP phrases:\r\n",
    "all_chunk_types = ['(NP', '(VP', '(PP','(SBAR','(ADJP','(PRT','(ADVP','(INTJ','(CONJP','(LST','(UCP','(PADDING','(UNAVAILABLE']\r\n",
    "\r\n",
    "all_phrases = []\r\n",
    "for type in all_chunk_types:\r\n",
    "    for size in range(3,10,2):\r\n",
    "        all_phrases.append(list(set(identify_phrases_size_n(corpus_less_dirty, \r\n",
    "                                                            chunk_type = type, \r\n",
    "                                                            chunk_size=size)))) # NP phrases\r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "75c4db28bb58e6de10e05be21b6046b5ba21d9aba4af4007d97c2f3325bc0896"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}