{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# POS and BIOES NN model stacking experiments:\r\n",
    "# (0) Library imports:-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from POSTaggerModel import RNNPOSTagger\r\n",
    "from ConLLBIOESTaggerModel import RNNBIOESTagger\r\n",
    "import torch\r\n",
    "import pickle, gzip\r\n",
    "from typing import Tuple, List"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (1) Step 1.1: Load all 8 dictionaries:-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# (1.1) POS dictionary from neural network #1:\r\n",
    "\r\n",
    "def load_pickles(filename):\r\n",
    "    data_raw_filepath = \"C:/Users/rahin/projects/nlp-pos-bioes-tagging/data/raw/\"\r\n",
    "\r\n",
    "    if \"pklz\" in filename:     \r\n",
    "        file = gzip.open(data_raw_filepath+str(filename),\"rb\")\r\n",
    "    else:\r\n",
    "        # print(f\"loading: {data_raw_filepath+str(filename)}\")\r\n",
    "        file = open(data_raw_filepath+str(filename),\"rb\")\r\n",
    "\r\n",
    "    return pickle.load(file)\r\n",
    "\r\n",
    "list_of_lookup_tables = [\"nn1_wsj_idx_to_pos.pkl\",\r\n",
    "                         \"nn1_wsj_pos_to_idx.pkl\",\r\n",
    "                         \"nn2_ConLL2003_pos_tags.pkl\",\r\n",
    "                         \"nn1_wsj_word_to_idx.pkl\",\r\n",
    "                         \"nn1_wsj_idx_to_word.pkl\",\r\n",
    "                         \"nn2_ConLL2003_vocabulary.pkl\",\r\n",
    "                         \"nn2_ConLL2003_BIOES_tags.pkl\"]\r\n",
    "# (a) Vocabulory: \r\n",
    "nn1_vocab = load_pickles(list_of_lookup_tables[3])\r\n",
    "nn1_vocab_reverse = load_pickles(list_of_lookup_tables[4])\r\n",
    "nn2_vocab = load_pickles(list_of_lookup_tables[5])\r\n",
    "nn2_vocab_reverse = {}\r\n",
    "for key,value in nn2_vocab.items():\r\n",
    "    nn2_vocab_reverse[value] = key\r\n",
    "\r\n",
    "# (b) POS tags:\r\n",
    "nn1_pos_tags = load_pickles(list_of_lookup_tables[0])\r\n",
    "nn1_pos_tags_reverse = load_pickles(list_of_lookup_tables[1])\r\n",
    "nn2_pos_tags_reverse = load_pickles(list_of_lookup_tables[2])\r\n",
    "nn2_pos_tags = {}\r\n",
    "for key,value in nn2_pos_tags_reverse.items():\r\n",
    "    nn2_pos_tags[value] = key\r\n",
    "\r\n",
    "# (c) BIOES tags:\r\n",
    "nn2_bioes_tags_reverse = load_pickles(list_of_lookup_tables[6])\r\n",
    "nn2_bioes_tags = {}\r\n",
    "for key,value in nn2_bioes_tags_reverse.items():\r\n",
    "    nn2_bioes_tags[value] = key\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# checking Vocab\r\n",
    "# print(list(nn1_vocab.keys())[:10])\r\n",
    "# print(list(nn1_vocab_reverse.keys())[:10])\r\n",
    "print(list(nn2_vocab.keys())[:10])\r\n",
    "# print(list(nn2_vocab_reverse.keys())[:10])\r\n",
    "\r\n",
    "# checking POS tags\r\n",
    "# print(list(nn1_pos_tags.keys())[:10])\r\n",
    "# print(list(nn1_pos_tags_reverse.keys())[:10])\r\n",
    "# print(list(nn2_pos_tags.keys())[:10])\r\n",
    "print(list(nn2_pos_tags_reverse.keys())[:10])\r\n",
    "\r\n",
    "# checking BIOES tags\r\n",
    "# print(list(nn2_bioes_tags.keys())[:10])\r\n",
    "print(list(nn2_bioes_tags_reverse.keys())[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Forecasts', '31.80', 'Johns', 'concluding', 'Seizinger', 'extraordinary', 'sorghum', 'shorter', 'adversories', 'Kamiel']\n",
      "['CC', 'NNPS', '$', 'VBP', 'VB', 'FW', 'RBR', 'PDT', 'LS', 'JJR']\n",
      "['I-LST', 'E-PRT', 'I-CONJP', 'E-ADJP', 'I-ADVP', 'E-INTJ', 'E-PP', 'B-CONJP', '-X-', 'B-ADVP']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (1) Step 1.2: Sync POS tags from both corpus': "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# (a) Check what needs to be replaced:\r\n",
    "in_penn_not_in_conll, in_conll_not_in_penn = [], []\r\n",
    "penn_tags = list(nn1_pos_tags_reverse.keys())\r\n",
    "conll_tags = list(nn2_pos_tags_reverse.keys())\r\n",
    "for item in penn_tags:\r\n",
    "    if item not in conll_tags:\r\n",
    "        in_penn_not_in_conll.append(item)\r\n",
    "for item in conll_tags:\r\n",
    "    if item not in penn_tags:\r\n",
    "        in_conll_not_in_penn.append(item)\r\n",
    "\r\n",
    "print(f\"Tag in Penn but not in Conll:\\n{in_penn_not_in_conll}\")\r\n",
    "print(f\"Tag in Conll but not in Penn:\\n{in_conll_not_in_penn}\")\r\n",
    "\r\n",
    "############################################################################\r\n",
    "\r\n",
    "# (b) Change in Tags -> IDX table\r\n",
    "nn2_pos_tags_reverse['-LRB-'] = nn2_pos_tags_reverse.pop('(')\r\n",
    "nn2_pos_tags_reverse['-RRB-'] = nn2_pos_tags_reverse.pop(')')\r\n",
    "nn2_pos_tags_reverse['``'] = nn2_pos_tags_reverse.pop('\"')\r\n",
    "nn2_pos_tags_reverse['#'] = nn2_pos_tags_reverse.pop('-X-')\r\n",
    "nn2_pos_tags_reverse['-NONE-'] = nn2_pos_tags_reverse.pop('NN|SYM')\r\n",
    "\r\n",
    "print(nn1_pos_tags_reverse.keys())\r\n",
    "print(nn2_pos_tags_reverse.keys())\r\n",
    "\r\n",
    "# (c) Change in Idx -> Tags table\r\n",
    "nn2_pos_tags['11'] = '#'\r\n",
    "nn2_pos_tags['3'] = '-RRB-'\r\n",
    "nn2_pos_tags['33'] = '-LRB-'\r\n",
    "nn2_pos_tags['45'] = '-NONE'\r\n",
    "nn2_pos_tags['29'] = '``'\r\n",
    "\r\n",
    "\r\n",
    "nn2_pos_tags.values()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tag in Penn but not in Conll:\n",
      "['``', '-RRB-', '#', '-LRB-']\n",
      "Tag in Conll but not in Penn:\n",
      "[')', '-X-', '\"', '(', 'NN|SYM']\n",
      "dict_keys(['NN', 'UH', 'PDT', ',', 'CD', '``', 'JJS', 'FW', 'RBS', 'WRB', 'WP$', 'NNPS', 'NNP', 'VBG', 'VBP', 'WP', 'NNS', 'VBD', ':', 'DT', 'CC', 'RB', 'RBR', '-RRB-', '.', \"''\", 'VB', 'PRP', 'EX', 'VBZ', 'IN', 'JJ', 'JJR', 'SYM', 'WDT', 'TO', 'POS', 'VBN', 'MD', 'RP', 'PRP$', '#', '-LRB-', 'LS', '$', 'PADDING'])\n",
      "dict_keys(['CC', 'NNPS', '$', 'VBP', 'VB', 'FW', 'RBR', 'PDT', 'LS', 'JJR', 'NNP', '.', 'WP$', ':', 'DT', 'NN', 'VBZ', 'EX', 'MD', 'RP', 'UH', 'VBD', 'PRP$', ',', 'WRB', 'TO', 'RBS', 'WDT', 'JJS', 'POS', 'VBG', 'PRP', 'WP', 'IN', 'NNS', 'SYM', \"''\", 'VBN', 'JJ', 'CD', 'RB', 'PADDING', '-LRB-', '-RRB-', '``', '#', '-NONE-'])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_values(['CC', 'NNPS', '$', ')', 'VBP', 'VB', 'FW', 'RBR', 'PDT', 'LS', 'JJR', '-X-', 'NNP', '.', 'WP$', ':', 'DT', 'NN', 'VBZ', 'EX', 'MD', 'RP', 'UH', 'VBD', 'PRP$', ',', 'WRB', 'TO', 'RBS', '\"', 'WDT', 'JJS', 'POS', '(', 'VBG', 'PRP', 'WP', 'IN', 'NNS', 'SYM', \"''\", 'VBN', 'JJ', 'CD', 'RB', 'NN|SYM', 'PADDING', '#', '-RRB-', '-LRB-', '-NONE', '``'])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# nn1_vocab # word to idx\r\n",
    "# nn1_pos_tags_reverse # Tags to idx\r\n",
    "len(nn1_pos_tags_reverse)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (2) Step 2: Load pre-trained model weights:-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# model 1: Hyperparameters:\r\n",
    "POS_VOCAB_SIZE = len(nn1_vocab)+1\r\n",
    "POS_EMBED_DIM = 100\r\n",
    "POS_HIDDEN_DIM = 32\r\n",
    "POS_NUM_LAYERS = 2\r\n",
    "POS_NUM_OF_CLASSES = len(nn1_pos_tags_reverse)\r\n",
    "POS_N_EPOCHS = 10\r\n",
    "POS_LEARNING_RATE = 0.02\r\n",
    "POS_BATCH_SIZE = 128\r\n",
    "# model 1: POS tagger\r\n",
    "pos_tagger_model = RNNPOSTagger(embedding_dimension = POS_EMBED_DIM,\r\n",
    "                    vocabulary_size = POS_VOCAB_SIZE,\r\n",
    "                    hidden_dimension = POS_HIDDEN_DIM,\r\n",
    "                    num_of_layers = POS_NUM_LAYERS,\r\n",
    "                    dropout = 0.1,\r\n",
    "                    output_dimension = POS_NUM_OF_CLASSES)\r\n",
    "pos_tagger_model.load_state_dict(torch.load(\"C:/Users/rahin/projects/nlp-pos-bioes-tagging/data/processed/PennPOSmodel.pth\"))  \r\n",
    "\r\n",
    "###########################################################################################\r\n",
    "# model 2: Hyperparameters:\r\n",
    "BIOES_VOCAB_SIZE = len(nn2_vocab)+len(nn2_pos_tags_reverse)+2\r\n",
    "BIOES_EMBED_DIM = 100\r\n",
    "BIOES_HIDDEN_DIM = 64\r\n",
    "BIOES_NUM_LAYERS = 2\r\n",
    "BIOES_NUM_OF_CLASSES = len(nn2_bioes_tags_reverse)+1\r\n",
    "BIOES_N_EPOCHS = 10\r\n",
    "BIOES_LEARNING_RATE = 0.01\r\n",
    "BIOES_BATCH_SIZE = 32\r\n",
    "\r\n",
    "################################### 02. NN Model  ########################################\r\n",
    "\r\n",
    "bioes_tagger_model = RNNBIOESTagger(embedding_dimension= BIOES_EMBED_DIM,\r\n",
    "                            vocabulary_size=BIOES_VOCAB_SIZE,\r\n",
    "                            hidden_dimension=BIOES_HIDDEN_DIM,\r\n",
    "                            num_of_layers=BIOES_NUM_LAYERS,\r\n",
    "                            dropout=0.2,\r\n",
    "                            output_dimension=BIOES_NUM_OF_CLASSES)\r\n",
    "bioes_tagger_model.load_state_dict(torch.load(\"C:/Users/rahin/projects/nlp-pos-bioes-tagging/data/processed/ConLLBIOESmodel.pth\"))                              "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "validation_dataset = load_pickles(\"PennTreeBankValid.pklz\")\r\n",
    "# validation_dataset[0] #everything is in text format"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Validation Dataset through nn 1 and 2:- \r\n",
    "## (3.1) Functions that converts Token & Tags --> Index and Index --> Token & Tags:- "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def token_pipeline(x):\r\n",
    "    \r\n",
    "    if len(x) < 50:\r\n",
    "        for i in range(0,50-len(x)):\r\n",
    "            x.append('PADDING')\r\n",
    "    return [nn1_vocab[tok] for tok in x]\r\n",
    "\r\n",
    "def token_reverse_pipeline(x):\r\n",
    "    return [nn1_vocab_reverse[idx] for idx in x]\r\n",
    "\r\n",
    "def pos_reverse_pipeline(x):\r\n",
    "    return [nn1_pos_tags[idx] for idx in x]\r\n",
    "\r\n",
    "def pos_pipeline(x):\r\n",
    "\r\n",
    "    if len(x) <50:\r\n",
    "        for i in range(0,50-len(x)):\r\n",
    "            x.append('PADDING')\r\n",
    "    return [nn1_pos_tags_reverse[pos] for pos in x]\r\n",
    "#######################################################################################################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (3.2) Function returns predicted tags of the validation dataset:-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "def predict_dataset(input_dataset) -> Tuple[List, List]:\r\n",
    "\r\n",
    "    # convert text to numbers after expanding sentence length to 50 elements\r\n",
    "    sentence_to_idx = token_pipeline(input_dataset) \r\n",
    "    idx_to_tensor = torch.tensor(sentence_to_idx).unsqueeze(1).T #input sentence as tensors {A}\r\n",
    "    \r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        output = pos_tagger_model(idx_to_tensor)\r\n",
    "        predicted_output_tensor = torch.argmax(output, dim=2) # predicted labels as tensors {B}\r\n",
    "        predicted_output = predicted_output_tensor.squeeze(1).tolist()[0]\r\n",
    "\r\n",
    "    predicted_output_tags = pos_reverse_pipeline(predicted_output)\r\n",
    "        \r\n",
    "    # return idx_to_tensor, predicted_output # return Tuple({A},{B})\r\n",
    "    return predicted_output_tags, input_dataset\r\n",
    "\r\n",
    "# tag the validation dataset:\r\n",
    "predictions_pos_tagger = []\r\n",
    "\r\n",
    "for idx, (sentence,label) in enumerate(validation_dataset):\r\n",
    "    predictions_pos_tagger.append(predict_dataset(sentence))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (3.3) File 1: POS Tags | Sentence tree construction:-"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "def write_line_to_file(newline):\r\n",
    "    with open(r\"C:\\Users\\rahin\\projects\\nlp-pos-bioes-tagging\\data\\interim\\pos_tags_tree.txt\",\"a\") as f:\r\n",
    "        f.write(newline+'\\n')\r\n",
    "        f.close()\r\n",
    "\r\n",
    "for idx, samples in enumerate(predictions_pos_tagger):\r\n",
    "    pos_tags_final = predictions_pos_tagger[idx][0]\r\n",
    "    tokens_final = predictions_pos_tagger[idx][1]\r\n",
    "    line = \"\"\r\n",
    "    for item in zip(pos_tags_final, tokens_final):\r\n",
    "        line = line + str(item) + ' '\r\n",
    "\r\n",
    "    write_line_to_file(line)\r\n",
    "# f.close()\r\n",
    "    \r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "75c4db28bb58e6de10e05be21b6046b5ba21d9aba4af4007d97c2f3325bc0896"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}