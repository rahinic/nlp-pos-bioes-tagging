{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "compositionNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPVziM+phFRCH35sJxABYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahinic/nlp-pos-bioes-tagging/blob/master/compositionNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-jJOQnE02k9"
      },
      "source": [
        "# Step 1: Dictionary and Dirty Dataset acquisition:-\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grWd2RpW0tpF",
        "outputId": "580b18a8-9ae9-4187-87ec-9c05a0c5655e"
      },
      "source": [
        "class compositionNetworksTagSets():\n",
        "\n",
        "    def dictionary():\n",
        "        # compose dictionary of possible sentence composition tags\n",
        "        composition_tags = ['NP', 'PP', 'VP', 'ADVP', 'SBAR', 'ADJP', 'ADVP', 'PRT', 'CONJP', 'INTJ', 'LST', 'UCP', 'PADDING', 'UNAVAILABLE']\n",
        "        composition_tags_lkp, composition_tags_lkp_rev = {}, {}\n",
        "        \n",
        "        for idx, tag in enumerate(composition_tags):\n",
        "            composition_tags_lkp[tag] = idx\n",
        "            composition_tags_lkp_rev[idx] = tag\n",
        "        \n",
        "        print(\"Dictionary ready!\")\n",
        "        return composition_tags_lkp, composition_tags_lkp_rev\n",
        "\n",
        "class compositionNetworksDataset():\n",
        "    # fetch datasets\n",
        "    def load_corpus(self,filename):\n",
        "        filepath = \"/content/\"\n",
        "        f = open(filepath+str(filename),\"r\")\n",
        "        content = f.readlines()\n",
        "\n",
        "        return content\n",
        "\n",
        "    def fetch_corpus(self,corpus_type):\n",
        "\n",
        "        print(f\"loading the {corpus_type} dataset\")\n",
        "        import sys\n",
        "\n",
        "        types = ['train','test','validation']\n",
        "        try:\n",
        "            if corpus_type not in types:\n",
        "                raise ValueError\n",
        "        except ValueError as error:\n",
        "            print('Invalid dataset type: possible values: \"train/test/validation\"')\n",
        "            sys.exit(1)\n",
        "\n",
        "        if corpus_type == \"train\":\n",
        "            dataset = self.load_corpus(filename=\"02-21.10way.clean.txt\")\n",
        "        elif corpus_type == \"test\":\n",
        "            dataset = self.load_corpus(filename=\"23.auto.clean.txt\")\n",
        "        elif corpus_type == \"validation\":\n",
        "            dataset = self.load_corpus(filename=\"22.auto.clean.txt\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def prepare_sentences(self,corpus_type):\n",
        "\n",
        "        corpus_dataset = self.fetch_corpus(corpus_type)\n",
        "        \n",
        "        \n",
        "\n",
        "        #import dict\n",
        "        dict, dict_rev = compositionNetworksTagSets.dictionary()\n",
        "        list_of_tags = list(dict.keys())\n",
        "\n",
        "        return corpus_dataset\n",
        "\n",
        "\n",
        "    def identify_phrases_size_n(self, find_chunks_from, chunk_type, chunk_size):\n",
        "\n",
        "    #collects NP phrases of size 2 and 1 (at the end of sentence)\n",
        "        list_of_phrases = []\n",
        "        n = chunk_size \n",
        "        for sentence in find_chunks_from: # iterate through all sentences\n",
        "            curr_sentence = sentence.split(' ')\n",
        "            \n",
        "            for idx, item in enumerate(curr_sentence): \n",
        "\n",
        "                # look if NP phrase exist in this sentence            \n",
        "                if item == chunk_type:\n",
        "                    NP_item_position = idx\n",
        "                    if '))' in curr_sentence[idx:idx+n][-1]:\n",
        "                        curr_phrase = curr_sentence[idx:idx+n] # n= 5\n",
        "                        \n",
        "                    else: continue #skipping chunks longer than 2 words\n",
        "                    \n",
        "                    # remove the end of sentence indicator:\n",
        "                    if curr_phrase[-1] == '.)))\\n':\n",
        "                        curr_phrase =  curr_phrase[:-2]\n",
        "\n",
        "                    list_of_phrases.append(tuple(curr_phrase))             \n",
        "                \n",
        "                # elif item != chunk_type: # skip until the next NP phrase in the same sentence\n",
        "                #     continue\n",
        "        return list_of_phrases\n",
        "\n",
        "ds = compositionNetworksDataset()\n",
        "dirty_sentences = ds.prepare_sentences(corpus_type=\"validation\")\n",
        "all_chunk_types = ['(NP', '(VP', '(PP','(SBAR','(ADJP','(PRT','(ADVP','(INTJ','(CONJP','(LST','(UCP','(PADDING','(UNAVAILABLE']\n",
        "\n",
        "all_phrases = []\n",
        "for type in all_chunk_types:\n",
        "    for size in range(3,10,2):\n",
        "        all_phrases.append(list(set(ds.identify_phrases_size_n(dirty_sentences, \n",
        "                                                            chunk_type = type, \n",
        "                                                            chunk_size=size)))) # NP phrases\n",
        "  \n",
        "all_phrases_final = list(filter(None, all_phrases))\n",
        "#############################################################################\n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading the validation dataset\n",
            "Dictionary ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd9oq7DP1oS0"
      },
      "source": [
        "\n",
        "# compositionNetworksTagSets = compositionNetworksTagSets()\n",
        "# compositionNetworksDataset \n",
        "import pickle, gzip\n",
        "from typing import Tuple, List\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class composeDataset(Dataset):\n",
        "\n",
        "    # Step 1: look-up tables:\n",
        "    def fetchLookupTables(self):\n",
        "        dict = compositionNetworksTagSets\n",
        "        composition_tags, composition_tags_reverse = dict.dictionary()\n",
        "\n",
        "        return composition_tags, composition_tags_reverse\n",
        "\n",
        "    def fetchWordsAndPOSLookupTables(self):\n",
        "        def load_pickles(filename):\n",
        "            data_raw_filepath = \"/content/\"\n",
        "\n",
        "            if \"pklz\" in filename:     \n",
        "                file = gzip.open(data_raw_filepath+str(filename),\"rb\")\n",
        "            else:\n",
        "                # print(f\"loading: {data_raw_filepath+str(filename)}\")\n",
        "                file = open(data_raw_filepath+str(filename),\"rb\")\n",
        "\n",
        "            return pickle.load(file)\n",
        "\n",
        "        list_of_lookup_tables = [\"nn1_wsj_idx_to_pos.pkl\",\n",
        "                         \"nn1_wsj_pos_to_idx.pkl\",\n",
        "                         \"nn1_wsj_word_to_idx.pkl\",\n",
        "                         \"nn1_wsj_idx_to_word.pkl\"]\n",
        "        # (a) Vocabulory: \n",
        "        nn1_pos = load_pickles(list_of_lookup_tables[0])\n",
        "        nn1_pos_reverse = load_pickles(list_of_lookup_tables[1])\n",
        "        nn1_vocab = load_pickles(list_of_lookup_tables[2])\n",
        "        nn1_vocab_reverse = load_pickles(list_of_lookup_tables[3])\n",
        "\n",
        "        return nn1_vocab, nn1_pos, nn1_vocab_reverse, nn1_pos_reverse\n",
        "        \n",
        "    # Step 2: fetch dirty dataset\n",
        "    def fetchDirtyDataset(self, dataset):\n",
        "        ds = compositionNetworksDataset()\n",
        "        dirty_sentences = ds.prepare_sentences(corpus_type=dataset)\n",
        "        all_chunk_types = ['(NP', '(VP', '(PP','(SBAR','(ADJP','(PRT','(ADVP','(INTJ','(CONJP','(LST','(UCP','(PADDING','(UNAVAILABLE']\n",
        "\n",
        "        all_phrases = []\n",
        "        for type in all_chunk_types:\n",
        "            for size in range(3,10,2):\n",
        "                all_phrases.append(list(set(ds.identify_phrases_size_n(dirty_sentences, \n",
        "                                                                    chunk_type = type, \n",
        "                                                                    chunk_size=size)))) # NP phrases\n",
        "        \n",
        "        all_phrases_final = list(filter(None, all_phrases))\n",
        "        return all_phrases_final\n",
        "\n",
        "    # Step 3: Create tuples of (label, sentence) in a neat way:\n",
        "\n",
        "    def cleanDirtyDataset(self,phrases):\n",
        "        sentences_clean = []\n",
        "        \n",
        "        for phrase in phrases:\n",
        "            \n",
        "            sentence_pretty = []\n",
        "\n",
        "            label = str(phrase[0]).replace('(','')\n",
        "            \n",
        "            sentence_ugly = list(phrase[1:])\n",
        "            \n",
        "            \n",
        "            for item in sentence_ugly:\n",
        "                token_less_ugly = str(item).replace('(','')\n",
        "                token_less_ugly = token_less_ugly.replace(')','')\n",
        "                token_less_ugly = token_less_ugly.replace('\\n','')\n",
        "                sentence_pretty.append(token_less_ugly)\n",
        "            \n",
        "            sentences_clean.append((label,sentence_pretty)) #tuple(label, sentence)\n",
        "\n",
        "        return sentences_clean\n",
        "            \n",
        "    # Step 4: Convert tokens to idx:\n",
        "    def prepareTensors(self,phrases):\n",
        "\n",
        "        # pipelines\n",
        "        def sample_word_pipeline(x):                        # word to idx\n",
        "            return [self.penn_vocab[tok] for tok in x]\n",
        "            \n",
        "        def sample_pos_pipeline(x):                         # pos to idx\n",
        "            return [self.penn_pos_tags[pos] for pos in x]\n",
        "\n",
        "\n",
        "\n",
        "        samples = [item for sublist in phrases for item in sublist]\n",
        "        all_samples = []\n",
        "        skipped = 0\n",
        "        for idx, sample in enumerate(samples):\n",
        "\n",
        "            if len(sample)>20:\n",
        "                continue\n",
        "            \n",
        "            label = sample[0]\n",
        "            # print(sample[1])\n",
        "\n",
        "            sentence, pos_tags = [], []\n",
        "\n",
        "            #split sentence and pos tags\n",
        "            for i in range(0,len(sample[1]),2):\n",
        "\n",
        "                try:\n",
        "                    pos_tags.append(sample[1][i])\n",
        "                    sentence.append(sample[1][i+1])\n",
        "                except IndexError:\n",
        "                    continue\n",
        "\n",
        "            if len(sentence)<20:\n",
        "                for i in range(0,20-len(sentence)):\n",
        "                    sentence.append('PADDING')\n",
        "                    pos_tags.append('PADDING')\n",
        "            \n",
        "            # token to idx conversion\n",
        "            \n",
        "            try:\n",
        "                sentence_to_idx = sample_word_pipeline(sentence)\n",
        "                pos_to_idx = sample_pos_pipeline(pos_tags)\n",
        "                label_to_idx = self.composition_tags[label]\n",
        "            except KeyError:\n",
        "                skipped = skipped+1\n",
        "                # print(f\"skipped {skipped} lines so far:\")\n",
        "                continue\n",
        "\n",
        "            final_sample = [a + b for a, b in zip(sentence_to_idx, pos_to_idx)]\n",
        "\n",
        "            sample_as_tensor = torch.tensor(final_sample)\n",
        "            label_as_tensor = torch.tensor(label_to_idx)\n",
        "\n",
        "            all_samples.append((label_as_tensor,sample_as_tensor))\n",
        "\n",
        "        return all_samples\n",
        "\n",
        "     \n",
        "############################################################################################################\n",
        "\n",
        "    def __init__(self, myDataset=None):\n",
        "\n",
        "        self.myDataset = myDataset\n",
        "        # print(self.myDataset)\n",
        "\n",
        "        # Step 1: look-up tables:\n",
        "        print(\"Loading Penn Treebank look-up tables...\")\n",
        "        self.composition_tags, self.composition_tags_reverse = self.fetchLookupTables()\n",
        "        self.penn_vocab, _, _, self.penn_pos_tags = self.fetchWordsAndPOSLookupTables()\n",
        "        \n",
        "        print(\"done!\")\n",
        "\n",
        "        # Step 2: Fetching Pre-processed dataset:\n",
        "        print(\"Pre-processing Penn Treebank dataset...\")\n",
        "        self.cleanedRawDataset = self.fetchDirtyDataset(dataset=self.myDataset)\n",
        "        self.prettyDataset = []\n",
        "        for dataset in self.cleanedRawDataset:\n",
        "            self.prettyDataset.append(self.cleanDirtyDataset(dataset))\n",
        "        print(\"done!\")\n",
        "\n",
        "        # Step 3: Convert the dataset to tensors\n",
        "        print(\"Transforming the dataset to tensors...\")\n",
        "        self.final_dataset = self.prepareTensors(self.prettyDataset)\n",
        "        print(\"done!\")\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.final_dataset)\n",
        "\n",
        "    def __getitem__(self, idx) :\n",
        "\n",
        "        return self.final_dataset[idx]\n",
        "            \n",
        "\n",
        "    \n",
        "###################################################################\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyTv9KJj2LbI"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "\"\"\"RNN Many-to-one multi-class classification neural network model framework design\"\"\"\n",
        "\n",
        "class RNNCompositionNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                embedding_dimension, \n",
        "                vocabulary_size,\n",
        "                hidden_dimension,\n",
        "                num_of_layers,\n",
        "                dropout,\n",
        "                output_dimension\n",
        "                ):\n",
        "        super(RNNCompositionNetwork, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                    embedding_dim=embedding_dimension)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dimension,\n",
        "                            hidden_dimension,\n",
        "                            num_of_layers,\n",
        "                            dropout=dropout,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dimension*2, output_dimension)\n",
        "\n",
        "        self.activation_fn = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, sample):\n",
        "        # print(sample.size())\n",
        "        embedded = self.embedding(sample)\n",
        "        # print(embedded.size())\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        # print(hidden.size())\n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-1,:,:], hidden[0,:,:]), dim = 1)\n",
        "        # print(hidden.size())\n",
        "\n",
        "        dense_output = self.fc(hidden)\n",
        "\n",
        "        #activation function\n",
        "        outputs=self.activation_fn(dense_output)\n",
        "\n",
        "        return dense_output"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y01DUu211_df",
        "outputId": "472c80c4-6110-465f-c372-76a3fdc0364f"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "# from dataset import composeDataset\n",
        "composeDataset = composeDataset\n",
        "# from dictionary import compositionNetworksTagSets\n",
        "# from model import RNNCompositionNetwork\n",
        "import pickle, time\n",
        "##########################################################################################\n",
        "################################# 01.TRain/Test Dataset ##################################\n",
        "\n",
        "train_dataset = DataLoader(dataset=composeDataset(myDataset=\"train\"),\n",
        "                                batch_size=16,\n",
        "                                shuffle=True)\n",
        "                                \n",
        "test_dataset = DataLoader(dataset=composeDataset(myDataset=\"test\"),\n",
        "                                batch_size=16,\n",
        "                                shuffle=True)\n",
        "##########################################################################################\n",
        "################################ 02. Look-up tables ######################################\n",
        "# (a) targets:-\n",
        "dict1 = compositionNetworksTagSets\n",
        "composition_tags, composition_tags_reverse = dict1.dictionary()\n",
        "# (b) samples and pos tags:-\n",
        "def fetchWordsAndPOSLookupTables():\n",
        "    def load_pickles(filename):\n",
        "        data_raw_filepath = \"/content/\"\n",
        "\n",
        "        if \"pklz\" in filename:     \n",
        "            file = gzip.open(data_raw_filepath+str(filename),\"rb\")\n",
        "        else:\n",
        "               # print(f\"loading: {data_raw_filepath+str(filename)}\")\n",
        "            file = open(data_raw_filepath+str(filename),\"rb\")\n",
        "\n",
        "        return pickle.load(file)\n",
        "\n",
        "    list_of_lookup_tables = [\"nn1_wsj_idx_to_pos.pkl\",\n",
        "                         \"nn1_wsj_pos_to_idx.pkl\",\n",
        "                         \"nn1_wsj_word_to_idx.pkl\",\n",
        "                         \"nn1_wsj_idx_to_word.pkl\"]\n",
        "        # (a) Vocabulory: \n",
        "    nn1_pos_reverse = load_pickles(list_of_lookup_tables[0])\n",
        "    nn1_pos = load_pickles(list_of_lookup_tables[1])\n",
        "    nn1_vocab = load_pickles(list_of_lookup_tables[2])\n",
        "    nn1_vocab_reverse = load_pickles(list_of_lookup_tables[3])\n",
        "\n",
        "    return nn1_vocab, nn1_pos, nn1_vocab_reverse, nn1_pos_reverse\n",
        "\n",
        "\n",
        "penn_vocab, _, _, penn_pos_tags = fetchWordsAndPOSLookupTables()\n",
        "\n",
        "##########################################################################################\n",
        "################################# 02.Model Parameters ####################################\n",
        "VOCAB_SIZE = len(penn_vocab)+len(penn_pos_tags)+1\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "NUM_LAYERS = 1\n",
        "NUM_OF_CLASSES = len(composition_tags)\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.02\n",
        "BATCH_SIZE = 64\n",
        "################################### 02. NN Model  #######################################\n",
        "print(\"03. builing the model...\")\n",
        "model = RNNCompositionNetwork(embedding_dimension= EMBED_DIM,\n",
        "                            vocabulary_size=VOCAB_SIZE,\n",
        "                            hidden_dimension=HIDDEN_DIM,\n",
        "                            num_of_layers=NUM_LAYERS,\n",
        "                            dropout=0,\n",
        "                            output_dimension=NUM_OF_CLASSES)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Done! here is our model:\")\n",
        "print(model)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "##########################################################################################\n",
        "############################# 03. Optimizer and Loss  #################################\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    # correct = (rounded_preds == y).float() \n",
        "    _,pred_label = torch.max(rounded_preds, dim = 1)\n",
        "\n",
        "    # sanity check:\n",
        "    # print(f\"Actual label: {y}\")\n",
        "    # print(rounded_preds)\n",
        "    # print(f\"Predicted label: {pred_label}\")\n",
        "    correct = (pred_label == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "#push to cuda if available\n",
        "# model = model.to(device)\n",
        "# criterion = criterion.to(device)\n",
        "\n",
        "##########################################################################################\n",
        "############################## 04. NN Model Train Definition #############################\n",
        "\n",
        "def train(model, dataset, optimizer, criterion):\n",
        "    \n",
        "    t = time.localtime()\n",
        "    start_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(start_time)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, sample in enumerate(dataset):\n",
        "       \n",
        "       current_samples = sample[1]\n",
        "       current_labels = sample[0]\n",
        "    #    print(current_samples[:5])\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       predicted_labels = model(current_samples)\n",
        "      #  print(predicted_label)\n",
        "\n",
        "       \n",
        "       loss = criterion(predicted_labels, current_labels)\n",
        "       accuracy = binary_accuracy(predicted_labels, current_labels)\n",
        "\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       epoch_loss += loss.item()\n",
        "       epoch_accuracy += accuracy.item()\n",
        "\n",
        "    return epoch_loss/len(dataset), epoch_accuracy/len(dataset)\n",
        "\n",
        "##########################################################################################\n",
        "################################ 05. NN Model Eval Definition ############################\n",
        "def evaluate(model, dataset, criterion):\n",
        "    \n",
        "    t = time.localtime()\n",
        "    start_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(start_time)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for idx, sample in enumerate(dataset):\n",
        "            current_samples = sample[1]\n",
        "            current_labels = sample[0]\n",
        "\n",
        "            predicted_labels = model(current_samples)\n",
        "\n",
        "            loss = criterion(predicted_labels, current_labels)\n",
        "            accuracy = binary_accuracy(predicted_labels, current_labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += accuracy.item()\n",
        "\n",
        "    return epoch_loss/len(dataset), epoch_accuracy/len(dataset)\n",
        "\n",
        "############################################################################################\n",
        "################################## 06. NN Model training #####################################\n",
        "# EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(epoch)\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_dataset, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, test_dataset, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Penn Treebank look-up tables...\n",
            "Dictionary ready!\n",
            "done!\n",
            "Pre-processing Penn Treebank dataset...\n",
            "loading the train dataset\n",
            "Dictionary ready!\n",
            "done!\n",
            "Transforming the dataset to tensors...\n",
            "done!\n",
            "Loading Penn Treebank look-up tables...\n",
            "Dictionary ready!\n",
            "done!\n",
            "Pre-processing Penn Treebank dataset...\n",
            "loading the test dataset\n",
            "Dictionary ready!\n",
            "done!\n",
            "Transforming the dataset to tensors...\n",
            "done!\n",
            "Dictionary ready!\n",
            "03. builing the model...\n",
            "----------------------------------------------------------------\n",
            "Done! here is our model:\n",
            "RNNCompositionNetwork(\n",
            "  (embedding): Embedding(46396, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=13, bias=True)\n",
            "  (activation_fn): ReLU()\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "0\n",
            "08:59:59\n",
            "09:03:03\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.622 | Train Acc: 83.94%\n",
            "\t Val. Loss: 0.628 |  Val. Acc: 82.13%\n",
            "-------------------------------------------------------------------\n",
            "1\n",
            "09:03:04\n",
            "09:06:08\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.542 | Train Acc: 83.89%\n",
            "\t Val. Loss: 0.708 |  Val. Acc: 79.57%\n",
            "-------------------------------------------------------------------\n",
            "2\n",
            "09:06:10\n",
            "09:09:14\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.506 | Train Acc: 83.89%\n",
            "\t Val. Loss: 0.527 |  Val. Acc: 81.91%\n",
            "-------------------------------------------------------------------\n",
            "3\n",
            "09:09:15\n",
            "09:12:24\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.453 | Train Acc: 84.17%\n",
            "\t Val. Loss: 0.570 |  Val. Acc: 82.24%\n",
            "-------------------------------------------------------------------\n",
            "4\n",
            "09:12:26\n",
            "09:15:35\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.407 | Train Acc: 84.92%\n",
            "\t Val. Loss: 0.454 |  Val. Acc: 83.26%\n",
            "-------------------------------------------------------------------\n",
            "5\n",
            "09:15:36\n",
            "09:18:45\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.377 | Train Acc: 85.74%\n",
            "\t Val. Loss: 0.432 |  Val. Acc: 84.71%\n",
            "-------------------------------------------------------------------\n",
            "6\n",
            "09:18:46\n",
            "09:21:58\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.346 | Train Acc: 87.53%\n",
            "\t Val. Loss: 0.401 |  Val. Acc: 86.19%\n",
            "-------------------------------------------------------------------\n",
            "7\n",
            "09:22:00\n",
            "09:25:17\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.315 | Train Acc: 88.75%\n",
            "\t Val. Loss: 0.387 |  Val. Acc: 87.03%\n",
            "-------------------------------------------------------------------\n",
            "8\n",
            "09:25:18\n",
            "09:28:36\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.345 | Train Acc: 88.69%\n",
            "\t Val. Loss: 0.613 |  Val. Acc: 82.13%\n",
            "-------------------------------------------------------------------\n",
            "9\n",
            "09:28:38\n",
            "09:31:52\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 0.529 | Train Acc: 83.92%\n",
            "\t Val. Loss: 0.475 |  Val. Acc: 82.73%\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFyqy3Lz9A5-"
      },
      "source": [
        "# Predictions and Model Accuracy Calculations:-"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiC-5F1a85Dy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b639a8c-4f47-445d-bd7c-9cd746ba40f6"
      },
      "source": [
        "# 1. fetch dataset and few examples\n",
        "validation_dataset = DataLoader(dataset=composeDataset(myDataset=\"validation\"),\n",
        "                                batch_size=16,\n",
        "                                shuffle=True)\n",
        "\n",
        "for idx, sample in enumerate(validation_dataset):\n",
        "  if idx>1:\n",
        "    break\n",
        "\n",
        "  examples = sample[1][:2]\n",
        "  actual_labels = sample[0][:2]\n",
        "  \n",
        "# 2. Model initialization and load weights\n",
        "\n",
        "VOCAB_SIZE = len(penn_vocab)+len(penn_pos_tags)+1\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "NUM_LAYERS = 1\n",
        "NUM_OF_CLASSES = len(composition_tags)\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.02\n",
        "BATCH_SIZE = 64\n",
        "################################### 02. NN Model  #######################################\n",
        "print(\"03. builing the model...\")\n",
        "model = RNNCompositionNetwork(embedding_dimension= EMBED_DIM,\n",
        "                            vocabulary_size=VOCAB_SIZE,\n",
        "                            hidden_dimension=HIDDEN_DIM,\n",
        "                            num_of_layers=NUM_LAYERS,\n",
        "                            dropout=0.2,\n",
        "                            output_dimension=NUM_OF_CLASSES)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Done! here is our model:\")\n",
        "print(model)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "# 3. Load weights\n",
        "model.load_state_dict(torch.load(\"/content/saved_weights.pt\"))\n",
        "model.eval()\n",
        "\n",
        "\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Penn Treebank look-up tables...\n",
            "Dictionary ready!\n",
            "done!\n",
            "Pre-processing Penn Treebank dataset...\n",
            "loading the validation dataset\n",
            "Dictionary ready!\n",
            "done!\n",
            "Transforming the dataset to tensors...\n",
            "done!\n",
            "03. builing the model...\n",
            "----------------------------------------------------------------\n",
            "Done! here is our model:\n",
            "RNNCompositionNetwork(\n",
            "  (embedding): Embedding(46396, 100)\n",
            "  (lstm): LSTM(100, 64, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=128, out_features=13, bias=True)\n",
            "  (activation_fn): ReLU()\n",
            ")\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNCompositionNetwork(\n",
              "  (embedding): Embedding(46396, 100)\n",
              "  (lstm): LSTM(100, 64, batch_first=True, dropout=0.2)\n",
              "  (fc): Linear(in_features=128, out_features=13, bias=True)\n",
              "  (activation_fn): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsUtW3KmAMGf",
        "outputId": "82a1f4d5-b455-491a-b3c2-6e2a889c8657"
      },
      "source": [
        "\n",
        "# 4. Make prediction\n",
        "print(examples)\n",
        "print(actual_labels)\n",
        "with torch.no_grad():\n",
        "  output = model(examples)\n",
        "  \n",
        "  rounded_preds = torch.round(output)\n",
        "  \n",
        "  _,pred_label = torch.max(rounded_preds, dim = 1)\n",
        "  \n",
        "  print(f\"Actual labels: {actual_labels}\")\n",
        "  print(f\"Predicted labels: {pred_label}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[22867, 35225, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393,\n",
            "         46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393],\n",
            "        [29180, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393,\n",
            "         46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393, 46393]])\n",
            "tensor([0, 0])\n",
            "Actual labels: tensor([0, 0])\n",
            "Predicted labels: tensor([0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-TMHUmb52tu"
      },
      "source": [
        "# Validation dataset Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZytE2hKL52Vc",
        "outputId": "4c0df3f7-0d2e-4bed-d47c-e93639edf39a"
      },
      "source": [
        "\n",
        "overall_accuracy = []\n",
        "total_samples = len(validation_dataset)*16\n",
        "print(total_samples)\n",
        "for idx, sample in enumerate(validation_dataset):\n",
        "  # print(len(sample[0]))\n",
        "  actual_label = sample[0]\n",
        "  with torch.no_grad():\n",
        "    predicted_label = model(sample[1])\n",
        "    rounded_preds = torch.round(predicted_label)\n",
        "    _, pred_label = torch.max(rounded_preds, dim=1)\n",
        "    # print('-'*100)\n",
        "    # print(actual_label)\n",
        "    # print(pred_label)\n",
        "    correct = (pred_label == actual_label).float()\n",
        "    # print(correct)\n",
        "    # print(torch.sum(correct))\n",
        "    # print(total_samples)\n",
        "    overall_accuracy.append(torch.sum(correct))\n",
        "    # print(torch.sum(correct))\n",
        "\n",
        "# finally:\n",
        "# print(overall_accuracy[:30])\n",
        "\n",
        "overall_accuracy_final = []\n",
        "for item in overall_accuracy:\n",
        "  overall_accuracy_final.append(int(item))\n",
        "\n",
        "print(f\"Total validation accuracy is: {(sum(overall_accuracy_final)/total_samples)*100}\")\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10656\n",
            "Total validation accuracy is: 87.41554054054053\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}